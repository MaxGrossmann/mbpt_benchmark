{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is optimized to run on Noctua2 (PC2 in Paderborn, Germany).\n",
    "\n",
    "It analyzes all calculations in a calculation directory and all database entries.\n",
    "\n",
    "We check if the calculation finished normally or if it crashed for various reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START USER INPUT \n",
    "\"\"\"\n",
    "\n",
    "calc_dir = \"../../questaal_calc\"\n",
    "db_dir = \"../../questaal_database\"\n",
    "job_name = \"benchmark\"\n",
    "\n",
    "\"\"\"\n",
    "END USER INPUT \n",
    "\"\"\"\n",
    "\n",
    "# external imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# internal imports\n",
    "from qsgw_workflow.utils.helper import load_db_entry\n",
    "\n",
    "# check that all needed directories exist\n",
    "if not os.path.exists(calc_dir):\n",
    "    sys.exit(\"The calculation directory does not exist!\")\n",
    "if not os.path.exists(db_dir):\n",
    "    sys.exit(\"The database directory does not exist!\")\n",
    "\n",
    "# list of all materials with a calculation directory\n",
    "mat_dirs = os.listdir(calc_dir)\n",
    "\n",
    "# list of all materials with a database entry\n",
    "db_files = os.listdir(db_dir)\n",
    "\n",
    "# get the job ids of all currently running jobs\n",
    "queue_str = os.popen(f\"squeue_pretty -n {job_name:s}\").read()\n",
    "pattern = re.compile(r\"^\\s*(\\d+)\", re.MULTILINE)\n",
    "job_ids = [int(id) for id in pattern.findall(queue_str)]\n",
    "\n",
    "# go through all calculation directories (and associated database entries)\n",
    "# (also find materials with a large BSE transition space) \n",
    "audit = {\n",
    "    \"pending_jobs\": [],\n",
    "    \"running_jobs\": [],\n",
    "    \"crashed_before_qpg0w0\": [],\n",
    "    \"crashed_during_qpg0w0\": [],\n",
    "    \"crashed_during_qsgw\": [],\n",
    "    \"crashed_during_qsgw^\": [],\n",
    "    \"finished\": [],\n",
    "}\n",
    "band_space = {}\n",
    "for mat in tqdm(mat_dirs, \"Going through all calculation directories\"):\n",
    "    # get a list of all files in the calculation directory of the material\n",
    "    files = os.listdir(os.path.join(calc_dir, mat))\n",
    "    # get all log files and find the one with the highest id (last job)\n",
    "    log_files = [f for f in files if \"slurm-\" in f and f.endswith(\"log\")]\n",
    "    if not log_files:\n",
    "        audit[\"pending_jobs\"].append(mat)\n",
    "        continue\n",
    "    log_ids = [int(re.findall(r\"\\d+\", f)[0]) for f in log_files]\n",
    "    log_idx = np.argmax(log_ids)\n",
    "    log_id = log_ids[log_idx]\n",
    "    log_file = log_files[log_idx]\n",
    "    # check if the calculation is still running\n",
    "    if log_id in job_ids:\n",
    "        audit[\"running_jobs\"].append(mat)\n",
    "        continue\n",
    "    # check if a database entry exists\n",
    "    if mat + \".json\" not in db_files:\n",
    "        audit[\"pending_jobs\"].append(mat)\n",
    "        continue\n",
    "    cse = load_db_entry(os.path.join(db_dir, mat + \".json\"))\n",
    "    if cse.parameters[\"finish\"] == True:\n",
    "        audit[\"finished\"].append(mat)\n",
    "        continue\n",
    "    if cse.parameters[\"qsgw_flag\"] == True:\n",
    "        if \"nv\" in cse.parameters and \"nc\" in cse.parameters:\n",
    "            band_space[mat] = [cse.parameters[\"nv\"], cse.parameters[\"nc\"]]\n",
    "        audit[\"crashed_during_qsgw^\"].append(mat)\n",
    "        continue\n",
    "    if \"gap_qpg0w0_soc\" in cse.data:\n",
    "        audit[\"crashed_during_qsgw\"].append(mat)\n",
    "        continue\n",
    "    if \"eps_kpts\" in cse.parameters:\n",
    "        audit[\"crashed_during_qpg0w0\"].append(mat)\n",
    "        continue\n",
    "    else:\n",
    "        audit[\"crashed_before_qpg0w0\"].append(mat)\n",
    "        continue\n",
    "\n",
    "# reports\n",
    "print(\"\\n----------------------------------------\\nAudit results:\\n----------------------------------------\\n\")\n",
    "print(f\"{'Total':27} | {len(mat_dirs):>5d}\")\n",
    "counter = 0\n",
    "for key, value in audit.items():\n",
    "    num_mats = len(value)\n",
    "    print(f\"{key:27} | {num_mats:>5d}\")\n",
    "    counter += num_mats\n",
    "print(f\"{'Sum (sanity check)':27} | {counter:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we analyze the reasons for the crashes\n",
    "keys = [\n",
    "    \"crashed_before_qpg0w0\",\n",
    "    \"crashed_during_qpg0w0\",\n",
    "    \"crashed_during_qsgw\",\n",
    "    \"crashed_during_qsgw^\",\n",
    "]\n",
    "error_audit = {}\n",
    "for key in keys:\n",
    "    mats = audit[key]\n",
    "    error_audit[key] = {\n",
    "        \"dft_kpt_conv_max_iter\": [],\n",
    "        \"eps_kpt_conv_max_iter\": [],\n",
    "        \"qsgw_kpt_conv_max_iter\": [],\n",
    "        \"pqmap_error\": [],\n",
    "        \"bloch_sum_error\": [],\n",
    "        \"basis_problem\": [],\n",
    "        \"out_of_memory\": [],\n",
    "        \"metals\": [],\n",
    "        \"timeout\": [],\n",
    "        \"bsw_parallelization_error\": [],\n",
    "        \"band_idx_problem\": [],\n",
    "        \"inv_problem\": [],\n",
    "        \"other_bse_crashes\": [],\n",
    "        \"other_crashes\": [],\n",
    "    }\n",
    "    for mat in mats:\n",
    "        # get a list of all files in the calculation directory of the material\n",
    "        files = os.listdir(os.path.join(calc_dir, mat))\n",
    "        # get all log files and find the one with the highest id (last job)\n",
    "        log_files = [f for f in files if \"slurm-\" in f and f.endswith(\"log\")]\n",
    "        log_ids = [int(re.findall(r\"\\d+\", f)[0]) for f in log_files]\n",
    "        log_idx = np.argmax(log_ids)\n",
    "        log_id = log_ids[log_idx]\n",
    "        log_file = log_files[log_idx]\n",
    "        # catch some \"normal\" crashes\n",
    "        if \"dft_kpt_conv_max_iter.txt\" in files:\n",
    "            error_audit[key][\"dft_kpt_conv_max_iter\"].append(mat)\n",
    "            continue\n",
    "        if \"eps_kpt_conv_max_iter.txt\" in files:\n",
    "            error_audit[key][\"eps_kpt_conv_max_iter\"].append(mat)\n",
    "            continue\n",
    "        if \"qsgw_kpt_conv_max_iter.txt\" in files:\n",
    "            error_audit[key][\"qsgw_kpt_conv_max_iter\"].append(mat)\n",
    "            continue\n",
    "        if \"pqmap_error.txt\" in files:\n",
    "            error_audit[key][\"pqmap_error\"].append(mat)\n",
    "            continue\n",
    "        if \"block_sum_error.txt\" in files:\n",
    "            error_audit[key][\"bloch_sum_error\"].append(mat)\n",
    "            continue\n",
    "        # check if the job used to much memory - check 1\n",
    "        job_eff = os.popen(f\"seff {log_id:d}\").read()\n",
    "        match_eff = re.search(r\"Memory Efficiency:\\s+(\\d+(?:\\.\\d+)?)%\", job_eff)\n",
    "        mem_efficiency = float(match_eff.group(1))\n",
    "        if mem_efficiency > 100:\n",
    "            error_audit[key][\"out_of_memory\"].append(mat)\n",
    "            continue\n",
    "        # check if the job used to much memory - check 2\n",
    "        with open(os.path.join(calc_dir, mat, f\"slurm-{log_id:d}.err\"), \"r\") as f:\n",
    "            err_str = f.read()\n",
    "        if \"oom_kill\" in err_str:\n",
    "            error_audit[key][\"out_of_memory\"].append(mat)\n",
    "            continue\n",
    "        # check if the job used to much memory - check 3\n",
    "        # there are cases where the out of memory error appears in the 'llmf' log file\n",
    "        if \"llmf\" in files:\n",
    "            with open(os.path.join(calc_dir, mat, \"llmf\"), \"r\") as f:\n",
    "                llmf_str = f.read()\n",
    "            if \"Out Of Memory\" in llmf_str:\n",
    "                error_audit[key][\"out_of_memory\"].append(mat)\n",
    "                continue\n",
    "        # parse the log file\n",
    "        with open(os.path.join(calc_dir, mat, log_file), \"r\") as f:\n",
    "            log_str = f.read()\n",
    "        if \"The calculated material appears to be a metal in the QSGW.\" in log_str:\n",
    "            error_audit[key][\"metals\"].append(mat)\n",
    "            continue\n",
    "        elif \"DUE TO TIME LIMIT ***\" in log_str:\n",
    "            error_audit[key][\"timeout\"].append(mat)\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(calc_dir, mat, \"lbsw-b1\")): # check the BSE log file\n",
    "            with open(os.path.join(calc_dir, mat, \"lbsw-b1\"), \"r\") as f:\n",
    "                bsw_str = f.read()\n",
    "            if \"OOM Killed\" in bsw_str:\n",
    "                error_audit[key][\"out_of_memory\"].append(mat)\n",
    "                continue\n",
    "            elif \"mkw4a: number of threads per group < 1\" in bsw_str:\n",
    "                cse = load_db_entry(os.path.join(db_dir, mat + \".json\"))\n",
    "                cse.parameters[\"qsgw_kpts\"]\n",
    "                error_audit[key][\"bsw_parallelization_error\"].append(mat)\n",
    "                continue\n",
    "            elif \"bse: any(num_valnsp > num_val_sp)\" in bsw_str:\n",
    "                error_audit[key][\"band_idx_problem\"].append(mat)\n",
    "                continue\n",
    "            elif \"Exit -1  matinv: degtrf\" in bsw_str:\n",
    "                error_audit[key][\"inv_problem\"].append(mat)\n",
    "                continue\n",
    "            elif \"bse: fractional occupancies not supported\" in bsw_str:\n",
    "                error_audit[key][\"metals\"].append(mat)\n",
    "                continue\n",
    "            else:\n",
    "                error_audit[key][\"other_bse_crashes\"].append(mat)\n",
    "                continue\n",
    "        else:\n",
    "            error_audit[key][\"other_crashes\"].append(mat)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we classified all materials (sanity check)\n",
    "print(f\"{'Class':27} | {'Audit Materials':20} | {'Error Classification':20}\")\n",
    "for key in keys:\n",
    "    counter = 0\n",
    "    for err_key in error_audit[key]:\n",
    "        counter += len(error_audit[key][err_key])\n",
    "    print(f\"{key:27} | {len(audit[key]):<20d} | {counter:<20d}\")\n",
    "    \n",
    "# reports\n",
    "for key in keys:\n",
    "    print(f\"\\n----------------------------------------\\n{key:27s}\\n----------------------------------------\\n\")\n",
    "    print(f\"{'Total':27} | {len(audit[key]):>5d}\")\n",
    "    for err_key in error_audit[key]:\n",
    "        print(f\"{err_key:27} | {len(error_audit[key][err_key]):>5d}\")\n",
    "        \n",
    "# save the outputs (used by 'restart_noctua.py')\n",
    "audit_dir = \"../audit/benchmark/\"\n",
    "os.makedirs(audit_dir, exist_ok=True)\n",
    "for key in keys:\n",
    "    with open(audit_dir + key + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(error_audit[key], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_check(vbm_idx, bs):\n",
    "    \"\"\"\n",
    "    Helper function to check if a band structure has artifacts.\n",
    "    Input:\n",
    "        vbm_idx:    Zero-based index of the valence band maximum\n",
    "        bs:         Band structure object (see 'qsgw.utils.helper')\n",
    "    Output:\n",
    "        flag:       False is everything is fine, True is there is an artifact\n",
    "    \"\"\"\n",
    "    flag = False\n",
    "    for path in bs[\"bs_paths\"]:\n",
    "        band = path[\"bands\"][:, vbm_idx + 1]\n",
    "        max_diff = np.max(np.abs(np.diff(band)))\n",
    "        if max_diff > 1: # eV\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    "        \n",
    "# get for band structure problems (mostly caused by interpolation errors)\n",
    "for mat in mat_dirs:\n",
    "    # check if a database entry exists\n",
    "    if mat + \".json\" not in db_files:\n",
    "        continue\n",
    "    cse = load_db_entry(os.path.join(db_dir, mat + \".json\"))\n",
    "    vbm_idx = cse.parameters[\"vbm_idx\"]\n",
    "    if cse.parameters[\"finish\"] == True:\n",
    "        bs = cse.data[\"bs_qsgwbse\"]\n",
    "        if bs_check(vbm_idx, bs):\n",
    "            print(f\"{mat:s} - QSGW^ band structure problem.\")\n",
    "    if cse.parameters[\"qsgw_flag\"] == True:\n",
    "        bs = cse.data[\"bs_qsgw\"]\n",
    "        if bs_check(vbm_idx, bs):\n",
    "            print(f\"{mat:s} - QSGW band structure problem.\")\n",
    "    if \"gap_qpg0w0_soc\" in cse.data:\n",
    "        bs = cse.data[\"bs_qpg0w0\"]\n",
    "        if bs_check(vbm_idx, bs):\n",
    "            print(f\"{mat:s} - QPG0W0 band structure problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the directory of the crashed materials \n",
    "base_dir = os.getcwd()\n",
    "for key in keys:\n",
    "    for err_key in error_audit[key]:\n",
    "        for mat in error_audit[key][err_key]:\n",
    "            os.chdir(os.path.join(calc_dir, mat))\n",
    "            os.system(\"touch meta bz.h5; rm -rf [0-9]*run meta mixm.mat mixsigma; lmgwclear\")\n",
    "            os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze how memory usage correlates with the number of sites and number of electrons in a structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START USER INPUT \n",
    "\"\"\"\n",
    "\n",
    "calc_dir = \"../../questaal_calc\"\n",
    "db_dir = \"../../questaal_database\"\n",
    "job_name = \"benchmark\"\n",
    "\n",
    "\"\"\"\n",
    "END USER INPUT \n",
    "\"\"\"\n",
    "\n",
    "# external imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# internal imports\n",
    "from qsgw_workflow.utils.helper import load_db_entry\n",
    "\n",
    "# check that all needed directories exist\n",
    "if not os.path.exists(calc_dir):\n",
    "    sys.exit(\"The calculation directory does not exist!\")\n",
    "if not os.path.exists(db_dir):\n",
    "    sys.exit(\"The database directory does not exist!\")\n",
    "\n",
    "# Noctua 2 has no LaTeX...\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "\n",
    "# go through all calculation directories (and associated database entries)\n",
    "memory_audit = {}\n",
    "db_files = os.listdir(db_dir)\n",
    "mat_dirs = os.listdir(calc_dir)\n",
    "for mat in tqdm(mat_dirs, \"Going through all materials\"):\n",
    "    if not os.path.exists(os.path.join(db_dir, mat + \".json\")):\n",
    "        continue\n",
    "    cse = load_db_entry(os.path.join(db_dir, mat + \".json\"))\n",
    "    num_sites = cse.structure.num_sites\n",
    "    num_elec = 0\n",
    "    for site in cse.structure:\n",
    "        num_elec += site.specie.Z\n",
    "    # get all log files and analyze the one with the highest id\n",
    "    files = os.listdir(os.path.join(calc_dir, mat))\n",
    "    log_files = [f for f in files if \"slurm-\" in f]\n",
    "    if not log_files:\n",
    "        continue\n",
    "    log_ids = [int(re.findall(r\"\\d+\", f)[0]) for f in log_files]\n",
    "    log_idx = np.argmax(log_ids)\n",
    "    log_id = log_ids[log_idx]\n",
    "    # check how much memory a job used\n",
    "    job_eff = os.popen(f\"seff {log_id:d}\").read()\n",
    "    match_eff = re.search(r\"Memory Efficiency:\\s+(\\d+(?:\\.\\d+)?)%\", job_eff)\n",
    "    mem_efficiency = float(match_eff.group(1))\n",
    "    match_mem = re.search(r\"Memory Utilized:\\s+([\\d\\.]+)\\s*(MB|GB|TB)\", job_eff)\n",
    "    memory_value = float(match_mem.group(1))\n",
    "    memory_unit = match_mem.group(2).upper() \n",
    "    if memory_unit == \"MB\":\n",
    "        memory_in_gb = memory_value / 1024\n",
    "    elif memory_unit == \"TB\":\n",
    "        memory_in_gb = memory_value * 1024\n",
    "    else: # already in GB\n",
    "        memory_in_gb = memory_value\n",
    "    memory_audit[mat] = {\n",
    "        \"cse\": cse,\n",
    "        \"num_sites\": num_sites,\n",
    "        \"num_elec\": num_elec,\n",
    "        \"memory_in_gb\": memory_in_gb,\n",
    "    }\n",
    "\n",
    "# makes plotting easier\n",
    "num_sites = []\n",
    "num_elecs = []\n",
    "memory = []\n",
    "for key in memory_audit.keys():\n",
    "    num_sites.append(memory_audit[key][\"num_sites\"])\n",
    "    num_elecs.append(memory_audit[key][\"num_elec\"])\n",
    "    memory.append(memory_audit[key][\"memory_in_gb\"])\n",
    "max_sites = max(num_sites)    \n",
    "\n",
    "# show how the number of sites and the number of electrons in the structure relate to the maximum memory used\n",
    "fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
    "ax = axes[0]\n",
    "ax.plot(num_sites, memory, \"o\")\n",
    "ax.set_xlabel(\"Number of Sites\")\n",
    "ax.set_ylabel(\"Max. Memory (GB)\")\n",
    "ax.set_xlim(0, max_sites+1)\n",
    "ax.set_xticks(np.arange(1, max_sites+1, 1))\n",
    "ax = axes[1]\n",
    "ax.plot(num_elecs, memory, \"o\")\n",
    "ax.set_xlabel(\"Number of Electrons\")\n",
    "ax.set_ylabel(\"Max. Memory (GB)\")\n",
    "ax.set_xlim(0, 400)\n",
    "ax.set_xticks([0, 100, 200, 300, 400])\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsgw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
